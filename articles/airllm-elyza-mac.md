---
title: "M1 Macã§AirLLMã‚’ä½¿ã£ã¦Elyza-13bã‚’å‹•ã‹ã—ã¦ã¿ãŸ" # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«
emoji: "ğŸ¦™" # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹çµµæ–‡å­—ï¼ˆ1æ–‡å­—ã ã‘ï¼‰
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢è¨˜äº‹
topics: ["macOS", "LLM", "llama"] # ã‚¿ã‚°ã€‚["markdown", "rust", "aws"]ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹
published: true # å…¬é–‹è¨­å®šï¼ˆfalseã«ã™ã‚‹ã¨ä¸‹æ›¸ãï¼‰
---

# ã¯ã˜ã‚ã«

å‰æ—¥2023å¹´ã®å¹´æœ«ã«ã€Elyzaã‹ã‚‰ã€Œæ—¢å­˜ã®ã‚ªãƒ¼ãƒ—ãƒ³ãªæ—¥æœ¬èªLLMã®ä¸­ã§æœ€é«˜æ€§èƒ½ã€ã¨è¨€ã‚ã‚Œã‚‹ã€ŒELYZA-japanese-Llama-2-13bã€ã‚·ãƒªãƒ¼ã‚ºãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã‚’ã€Œ70Bã®ãƒ¢ãƒ‡ãƒ«ã‚‚ 4GB GPU ã‚«ãƒ¼ãƒ‰ã§æ¨è«–ã§ãã‚‹ã€ã¨ã†ãŸã£ã¦ã„ã‚‹AirLLMã‚’ä½¿ã†ã“ã¨ã§ã€M1 Mac (MacBook Air M1 16GB)ã§å‹•ã‹ã—ã¦ã¿ã¾ã—ãŸã€‚

# ELYZA-japanese-Llama-2-13bã‚·ãƒªãƒ¼ã‚º

ã“ã‚Œã¯Meta ç¤¾ã®ã€ŒLlama 2ã€ã‚·ãƒªãƒ¼ã‚ºã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®è¿½åŠ å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ç¾¤ã§ã™ã€‚
ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯ Llama 2 Community License ã«æº–æ‹ ã—ã¦ãŠã‚Šã€Acceptable Use Policy ã«å¾“ã†é™ã‚Šã«ãŠã„ã¦ã¯ã€ç ”ç©¶ãŠã‚ˆã³å•†æ¥­ç›®çš„ã§ã®åˆ©ç”¨ãŒå¯èƒ½ã§ã™ã€‚

- [130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã€ŒLlama 2ã€ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸæ—¥æœ¬èªLLMã€ŒELYZA-japanese-Llama-2-13bã€ã‚’å…¬é–‹ã—ã¾ã—ãŸ](https://note.com/elyza/n/n5d42686b60b7)

ä»Šå›ã¯ã‚·ãƒªãƒ¼ã‚ºã®ä¸­ã‹ã‚‰ã€æ—¥æœ¬èªèªå½™ã®è¿½åŠ å­¦ç¿’ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åŠ¹ç‡åŒ–ã€ãã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¾“ã†è¿½åŠ å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚

- [ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)

# AirLLM

AirLLMã¯ã€å·¨å¤§ãªLLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã„GPUã§å®Ÿè¡Œå¯èƒ½ã«ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ãƒ¡ãƒ¢ãƒªã«è¼‰ã›ã‚‹ã®ã§ã¯ãªãã€è¨ˆç®—ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«åˆ†å‰²ã—ã¦ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰GPUãƒ¡ãƒ¢ãƒªã«è¼‰ã›ã‚‹ã“ã¨ã§ã€å°‘ãªã„GPUãƒ¡ãƒ¢ãƒªã§å®Ÿè¡Œå¯èƒ½ã«ã—ã¦ã„ã‚‹ã‚ˆã†ã§ã™ï¼ˆãã®åˆ†ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚Šã€å®Ÿè¡Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰

- [Unbelievable! Run 70B LLM Inference on a Single 4GB GPU with This NEW Technique](https://ai.gopubby.com/unbelievable-run-70b-llm-inference-on-a-single-4gb-gpu-with-this-new-technique-93e2057c7eeb)
- [GitHub lyogavin/Anima/air_llm](https://github.com/lyogavin/Anima/tree/main/air_llm)

ä»Šå›å¯¾è±¡ã¨ã—ã¦ã„ã‚‹13Bã®ãƒ¢ãƒ‡ãƒ«ãªã‚‰ã€ååˆ†å®Ÿè¡Œå¯èƒ½ãªã¯ãšã§ã™ã€‚

ã¾ãŸAirLLMã¯Apple siliconã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã®ã‚‚ç‰¹å¾´ã§ã™ã€‚ã€ŒMLXã€ã¨ã„ã†Apple siliconç”¨ã®NumPyãƒ©ã‚¤ã‚¯ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚

- [MLX](https://ml-explore.github.io/mlx/build/html/index.html)
  - [GitHub](https://github.com/ml-explore/mlx)

MLXã‚’ä½¿ã†å ´åˆã¯å¾®å¦™ã«æ›¸ãæ–¹ãŒå¤‰ã‚ã‚‹ã‚ˆã†ã§ã™ã€‚ã“ã¡ã‚‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒå‚è€ƒã«ãªã‚Šã¾ã™ã€‚

- [run_on_macos.ipynb](https://github.com/lyogavin/Anima/blob/main/air_llm/examples/run_on_macos.ipynb)

# å‹•ã‹ã—ã¦ã¿ã‚‹

Python 3.11.7 ã§å‹•ä½œã•ã›ã¦ã„ã¾ã™ã€‚

## ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```
pip install torch torchvision
pip install mlx

pip install airllm
```

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

```py
# elyze_airllm_mac.py

from airllm import AutoModel
import mlx.core as mx
import time

# ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®š
#model_name = "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct"
model_name = "elyza/ELYZA-japanese-Llama-2-13b-fast-instruct"

MAX_LENGTH = 128
MAX_NEW_TOKENS = 20

# ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
model = AutoModel.from_pretrained(model_name)


# -- å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ --
# input_text = [
#     'å¯Œå£«å±±ã®é«˜ã•ã¯ï¼Ÿ'
# ]
input_text = 'å¯Œå£«å±±ã®é«˜ã•ã¯ï¼Ÿ'

# -- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º --
def tokenize(model, text):
    input_ids = model.tokenizer(text,
        # return_tensors="pt", # NG
        return_tensors="np", # OK

        return_attention_mask=False,
        truncation=True,
        max_length=MAX_LENGTH,
        #padding=False
    )
    return input_ids

# -- ç”Ÿæˆ --
def generate(model, input_ids):
    generation_output = model.generate(
        mx.array(input_ids['input_ids']),
        max_new_tokens=MAX_NEW_TOKENS,
        use_cache=True,
        return_dict_in_generate=True
    )
    return generation_output

# -- æ¨è«–ã®å®Ÿè¡Œ --
def query(model, text):
    input_ids = tokenize(model, text)
    generation_output = generate(model, input_ids)
    return generation_output

# --- main ---
start = time.process_time()
output = query(model, input_text)
end = time.process_time()
print('----------------')
print(output)
print('--- ', end - start, ' sec ---')

```

## å®Ÿè¡Œ

åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚ç§ã®ç’°å¢ƒã§ã¯æ•°ååˆ†ã‹ã‹ã‚Šã¾ã—ãŸã€‚

- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å ´æ‰€: ~/.cache/huggingface/hub/_ãƒ¢ãƒ‡ãƒ«å_

çµæœã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```
- å¯Œå£«å±±ã®é«˜ã•ã¯3776.12 mã§ã™ã€‚</s><s>
---  287.62618599999996  sec ---
```

å¾®å¦™ã«å‰å¾Œã«ä½™è¨ˆãªã‚‚ã®ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ãŒã€å‹•ã‹ã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚å®Ÿè¡Œæ™‚é–“ã¯20ãƒˆãƒ¼ã‚¯ãƒ³ã§5åˆ†å¼±ã¨ã„ã†ã“ã¨ã§ã€å®Ÿç”¨çš„ã«ã¯å•é¡Œã‚ã‚Šã§ã™ã€‚

#  çµ‚ã‚ã‚Šã«

AirLLMã®ãŠã‹ã’ã§ã€è‡ªåˆ†ã®M1 Macã§ã‚‚é«˜æ€§èƒ½ã®LLMã‚’å‹•ã‹ã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚å®Ÿè¡Œæ™‚é–“ã¯ã¾ã ã¾ã ã§ã™ãŒã€ä»Šå¾Œã‚‚æ§˜ã€…ãªæ‰‹æ³•/ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå‡ºã¦ãã¦ã€æ™®é€šã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ã§ã‚‚å¿«é©ã«åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹æ—¥ãŒæ¥ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚

## é–¢é€£è¨˜äº‹

- [M1 Macã§llama.cppã‚’ä½¿ã£ã¦Elyza-13bã‚’å‹•ã‹ã—ã¦ã¿ãŸ](https://zenn.dev/mganeko/articles/llamacpp-elyza-mac)
